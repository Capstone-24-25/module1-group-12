---
title: "Biomarkers of ASD"
author: "Mai Uyen Huynh, Valerie De La Fuente, Reese Karo, Ivan Li"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
library(ggplot2)
library(readr)
library(dplyr)
library(reshape2)
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
library(kableExtra)
library(knitr)

# Read in the data
setwd('../data')
biomarker_raw <- read_csv('biomarker-raw.csv', show_col_types=FALSE)

# Set seed for reproducibility
set.seed(123)
```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

We are working with two datasets, `biomarker-raw.csv` and `biomarker-clean.RData`. Both datasets consist of serum samples of 76 boys who have autism spectrum disorder (ASD) and 78 boys who are typically developing (TD). Serum samples were obtained via fasting blood draw in a 3.5 ml Serum Separation Tube at times between 8AM-10AM for all subjects. The boys are of 18 months to 8 years of age. Within each subject, 1,317 proteins are measured, and the data helps to identify early biological markers for ASD. The average age for both groups is roughly the same. The mean ASD group age is 5.6 years old with a standard deviation of 1.7 years, and the mean TD group age is 5.7 years old with a standard deviation of 2 years. It is important to note that 41.7% of ASD subjects and 22.4% of TD subjects have seasonal allergies, which may have an effect on the data. For the 76 ASD subjects, the ratios of the ethnicities are as follows: 45.2% White/Caucasians, 35.6% Hispanic/Latinos, 4.1% African American, 2.6% Asian, 12.3% Multiple ethnicities/or Other, and 4.1% not reported. For the 78 TD subjects, the ratios of the ethnicities are as follows: 51.9% White/Caucasians, 7.8% Hispanic/Latinos, 18.2% African American, 3.9% Asian, 18.2% Multiple ethnicities/or Other, and 1.2% not reported.

`biomarker-raw.csv` contains all raw data that has been collected, with subjects row-wise and group, Autism Diagnostic Observation Schedule (ADOS), and proteins column-wise. `biomarker_clean` is the cleaned version of `biomarker-raw.csv` with the raw data log-transformed and z-transformed and outliers trimmed.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

In the sub-headers below, the data explains the processes of collecting data on the outliers in the dataset, on subjects and finding a different panel of proteins that compares to the benchmark from class.

### Impact of preprocessing and outliers

```{r}
# Subset all proteins in the dataset and convert to numeric values
proteins_numeric <- biomarker_raw %>% filter(Group %in% c("ASD", "TD")) %>%
  dplyr::select(contains("protein", ignore.case= TRUE)) %>% 
  mutate(across(everything(), as.numeric))

# Randomly sample a vector of 10 protein names
random_sample <- sample(names(proteins_numeric), 10)
```

```{r}
# Looking at distribution of raw values for the random sample of proteins

# Plot 10 histograms; one histogram per randomly sampled protein
proteins_numeric %>% 
  select(random_sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) + geom_histogram() +
  facet_wrap(vars(name))+
  labs(title = 'Distribution of Raw Values for Protein')
```

```{r}
# Looking at distribution of raw values for the random sample of proteins that are log-transformed

  # Log-transform all proteins
  proteins_numeric_log <- proteins_numeric %>%
    mutate(across(everything(), ~ log(. + 1)))
  
  # Plot 10 histograms; one histogram per randomly sampled protein

proteins_numeric_log %>% 
  select(random_sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) + geom_histogram() +
  facet_wrap(vars(name))+
  labs(title = 'Distribution of Log-Transformed Values')
```

We used histograms to apply exploratory data analysis on 10 randomly sampled proteins. Viewing their distributions, we can see that all of the data is right-skewed. This indicates that there are some individual outliers that contain a higher level of that protein. The 'secreted frizzled-related protein' is the only protein in the random sample that has a bit more of a symmetrical distribution. After log-transforming that same random sample of proteins and viewing their distributions, the data is much more symmetrical. When working with data that is asymmetrical, applying a log-transformation can normalize the data, improve model fit, and reduce outliers.

From there, we wanted to take a look at which group these individuals with outliers were coming from. To analyze this, we worked with any values that were equal to 3 or -3, since any outliers in the data were trimmed to those values. After visualizing the outlier count per group (ASD, TD) in a box plot and tabling the data, we see there is a total of 75 subjects with at least one outlier in the ASD group, and 77 in the TD group. However, when we visualise this with a box plot, we see there is a denser set of outliers in the TD class compared to the ASD class, which implies that there are more subjects in TD with a high amount of outliers compared to ASD. TD also has a higher amount of total outliers as a group (ASD 1007, TD 1372) and of the subjects with the top 10 most outliers, 7 are from the TD group and 3 are from the ASD group, with the top 2 spots from the TD group. In conclusion, while many subjects have at least 1 outlier, the subjects with a very high amount of outliers mostly belong to the TD group and the subject with the msot outliers is in the TD group.

```{r}
setwd('../data')
load(file = 'biomarker-clean.RData')
temp <- biomarker_clean %>% 
  filter(if_any(CHIP:PLXB2, ~ . == 3) | if_any(CHIP:PLXB2, ~ . == -3) )


outlier_count <- temp %>% 
  rowwise() %>%
  mutate(outliers = sum(c_across(CHIP:PLXB2) == 3) + sum(c_across(CHIP:PLXB2) == -3)) %>% 
  select(group, ados,outliers, everything())


#box plot of number of outliers per subject in each group
outlier_count %>% 
  ggplot(aes(x = group, y = outliers)) +
  geom_boxplot() +
  labs(title = 'Outliers per individual by group',
       x = 'Group',
       y = 'Number of outliers') 

#total outliers per group and number of people with outliers per group
outlier_count %>% 
  group_by(group) %>%
  summarise(total_out = sum(outliers), subjects_out = n()) %>% 
  kable(col.names = c('Group', 'Total Outliers', 'Number of Subjects'), caption = 'Total Outlier and Number of Subjects with Outliers per Group')

#top 10 subjects with most outliers
outlier_count %>% 
  arrange(desc(outliers)) %>% 
  select(group, outliers) %>%
  head(10) %>% 
  kbl(caption = 'Top 10 Subjects with Most Outliers')
```

### Methodlogical variations

To improve on the results of the prior analysis, we split the data into a training and testing set at the beginning, and ran the t-tests and Random Forest using only the testing set to find the top predictive proteins. Then, we evaluated the errors on the test set. This ensures that the logistic regression model is not overfit. After some experimentation, we found that the optimal choice for the number of top predictive proteins from each method was 20, as opposed to the previous 10. Furthermore, using a fuzzy intersection over a hard intersection allows for other potentially significant proteins to also be included into the panel. Our implementation involved picking the top 10 proteins from each list, combining them to form the new panel, and removing the duplicate proteins.

```{r}
# Split the data into training and testing; 80% training, 20% testing
biomarker_clean_split <- biomarker_clean %>%
  initial_split(prop = 0.8)

biomarker_clean_train <- training(biomarker_clean_split)

biomarker_clean_test <- testing(biomarker_clean_split)
```

```{r}
# Set a seed for reproducibility
set.seed(123)
# Finding our top 30 predictive proteins from each method

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean_train %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select 30 significant proteins from t-test
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 20) %>%
  pull(protein)

## RANDOM FOREST
##################

# Using only the training set, store predictors and response separately
predictors <- biomarker_clean_train %>%
  select(-c(group, ados))

response <- biomarker_clean_train %>% pull(group) %>% factor()

# fit RF
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores, selecting 30 significant proteins from RF
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 20) %>%
  pull(protein)
```

```{r}
# Fuzzy intersection; pick top 10 from each list and combine 
proteins_s1_ranked <- data.frame(proteins_s1, rank_s1=c(1:length(proteins_s1)))
proteins_s2_ranked <- data.frame(proteins_s2, rank_s2=c(1:length(proteins_s2)))

colnames(proteins_s1_ranked) <- c("name", "rank_s1")
colnames(proteins_s2_ranked) <- c("name", "rank_s2")

# Set the number of top proteins from each list
n <- 10  # adjustable

# Select the top proteins from each ranked list
top_s1 <- head(proteins_s1_ranked$name, n)
top_s2 <- head(proteins_s2_ranked$name, n)

# Combine the names and remove duplicates
proteins_sstar_fuzzy <- unique(c(top_s1, top_s2))

# View the fuzzy intersection panel
proteins_sstar_fuzzy
```

```{r}
# hard intersection
proteins_sstar_hard <- intersect(proteins_s1, proteins_s2)

# View the hard intersection panel
proteins_sstar_hard
```

```{r}
# panel from prior analysis
proteins_sstar_prior <- c("DERM", "RELT", "IgD", "FSTL1")
```

```{r}
# Set a seed for reproducibility
set.seed(123)
# Run the Logistic regression for all 3 panels

# Define the list of vectors to iterate through
protein_vectors <- list(
  hard = proteins_sstar_hard,
  fuzzy = proteins_sstar_fuzzy,
  prior = proteins_sstar_prior
)

# Initialize a list to store kable objects for each panel
kable_results <- list()


for (name in names(protein_vectors)) {
  

  current_proteins <- protein_vectors[[name]]
  
  # Preprocess data and partition
  biomarker_sstar <- biomarker_clean %>%
    select(group, any_of(current_proteins)) %>%
    mutate(class = (group == 'ASD')) %>%
    select(-group)
  
  biomarker_split <- biomarker_sstar %>%
    initial_split(prop = 0.8)

  biomarker_training <- training(biomarker_split)
  biomarker_testing <- testing(biomarker_split)
  
  # Fit logistic regression model
  fit <- glm(class ~ ., 
             data = biomarker_training, 
             family = 'binomial')
  
  # Evaluate errors on test set
  class_metrics <- metric_set(sensitivity, 
                              specificity, 
                              accuracy,
                              roc_auc)
  
  # Calculate metrics and create kable for each protein vector
  metrics_result <- biomarker_testing %>%
    add_predictions(fit, type = 'response') %>%
    mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
    class_metrics(estimate = est,
                  truth = tr_c, pred,
                  event_level = 'second')
  
  # Store the kable object
  kable_results[[name]] <- kable(metrics_result, caption = paste("Metrics for", name, "proteins"))
}

```

```{r}
# Prior panel results
kable_results[3]
```

The metric we are interested in is the accuracy of the logistic regression model. Note that the prior analysis did not partition the data into training and testing sets before running the t.tests and Random Forest. Because of this, the model from the prior was likely overfit, which is why it produced an impressive accuracy score of about 83.9%. After partitioning the data, it performed significantly worse on the testing set. We observe that the accuracy is about 67.7%. This indicates that the prior panel is can classify ASD correctly about 67.7% of the time on the testing set.

```{r}
# Fuzzy intersection panel results
kable_results[2]
```

When using a fuzzy intersection, the accuracy increased slightly, with a score of about 77.4%, when compared to the prior panel. This indicates that the fuzzy intersection panel is can classify ASD correctly only about 77.4% of the time on the testing set.

```{r}
# Hard intersection panel results
kable_results[1]
```

When using a hard intersection, the accuracy increased as well, with a score of about 70.9% when compared to the prior panel. This indicates that the hard intersection panel is can classify ASD correctly only about 70.9% of the time on the testing set.

### Improved classifier

After running a logistic regression on the prior, fuzzy intersection, and hard intersection panels, we improved the accuracy of the prior analysis by about 10%. The panel is the following.

```{r}
proteins_sstar_hard
```

This improvement of accuracy is mainly due to the splitting of the training and testing data before running the t-test and random forest to find significant proteins. The prior analysis likely overfit the model due to the lack of any splitting.

We have also achieved similar accuracy, at 70.9% (compared to 67.7%) with a fuzzy intersection. The panel is the following.

```{r}
proteins_sstar_fuzzy
```
